import streamlit as st
import pandas as pd
import re
from utils.mcp_client import MCPClient
from utils.config_manager import ConfigManager
from utils.llm_client import LLMClient

st.set_page_config(page_title="æ™ºèƒ½èŠå¤©", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ æ™ºèƒ½èŠå¤©")

# åˆå§‹åŒ–å®¢æˆ·ç«¯å’Œé…ç½®ç®¡ç†å™¨
mcp_client = MCPClient()
config_manager = ConfigManager()

# åˆå§‹åŒ–èŠå¤©å†å²å’Œåˆ†æçŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "analysis_plan" not in st.session_state:
    st.session_state.analysis_plan = None
if "analysis_question" not in st.session_state:
    st.session_state.analysis_question = None

# åˆ›å»ºè¾¹æ 
with st.sidebar:
    st.header("è®¾ç½®")
    
    # æ•°æ®åº“é€‰æ‹©
    database_type = st.selectbox("é€‰æ‹©æ•°æ®åº“", ["mysql", "athena"])
    
    # åŠ è½½æ•°æ®åº“é…ç½®
    db_config = config_manager.load_database_config().get(database_type, {})
    if not db_config:
        st.warning(f"è¯·å…ˆåœ¨æ•°æ®åº“é…ç½®é¡µé¢é…ç½®{database_type.upper()}è¿æ¥ä¿¡æ¯")
    
    # åŠ è½½LLMé…ç½®
    llm_config = config_manager.load_llm_config()
    
    # LLMæ¨¡å‹é€‰æ‹©
    st.subheader("LLMæ¨¡å‹è®¾ç½®")
    provider = st.selectbox(
        "LLMæä¾›å•†",
        ["openai", "azure_openai", "custom"],
        index=["openai", "azure_openai", "custom"].index(llm_config.get("provider", "openai"))
    )
    
    # æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„æ¨¡å‹
    if provider == "openai":
        model = st.selectbox(
            "æ¨¡å‹",
            ["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"],
            index=["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"].index(llm_config.get("openai", {}).get("model", "gpt-4"))
        )
        st.info(f"å½“å‰ä½¿ç”¨: OpenAI - {model}")
    elif provider == "azure_openai":
        deployment = llm_config.get("azure_openai", {}).get("deployment_name", "")
        st.info(f"å½“å‰ä½¿ç”¨: Azure OpenAI - {deployment}")
    else:  # custom
        model = llm_config.get("custom", {}).get("model", "llama2")
        st.info(f"å½“å‰ä½¿ç”¨: è‡ªå®šä¹‰ - {model}")
    
    # å…¶ä»–è®¾ç½®
    st.subheader("æ˜¾ç¤ºè®¾ç½®")
    show_schema = st.checkbox("æ˜¾ç¤ºSchemaæç¤º", value=True)
    use_llm = st.checkbox("ä½¿ç”¨LLMç”ŸæˆSQL", value=True)
    
    st.subheader("å®‰å…¨è®¾ç½®")
    check_dangerous_sql = st.checkbox("é¿å…æ‰§è¡Œå±é™©ä»£ç ", value=True)
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    llm_client = LLMClient(llm_config)

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "data" in message:
            st.dataframe(message["data"])

# è·å–ä¿å­˜çš„è¡¨ç»“æ„ä¿¡æ¯
def get_saved_schema(config_manager, database_type):
    schema_config = config_manager.load_schema_config().get(database_type, {})
    return schema_config.get("tables", {}), schema_config.get("descriptions", {})

# æ„å»ºåŒ…å«schemaçš„prompt
def build_schema_prompt(question, schema_info, table_descriptions):
    prompt = f"""### æ•°æ®åº“æŸ¥è¯¢

ç”¨æˆ·é—®é¢˜: {question}

### æ•°æ®åº“Schema:
"""
    
    for table, columns in schema_info.items():
        table_desc = table_descriptions.get(table, "")
        prompt += f"\n\nè¡¨: {table}"
        if table_desc:
            prompt += f" - {table_desc}"
        prompt += "\n"
        
        if columns:
            prompt += "| åˆ—å | ç±»å‹ | æè¿° |\n"
            prompt += "| --- | --- | --- |\n"
            for col in columns:
                name = col.get("name", "")
                col_type = col.get("type", "")
                comment = col.get("comment", "")
                prompt += f"| {name} | {col_type} | {comment} |\n"
        else:
            prompt += "è¡¨ç»“æ„ä¿¡æ¯æœªé…ç½®\n"
    
    prompt += "\n\nè¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæ•°æ®åº“schemaç”ŸæˆSQLæŸ¥è¯¢ã€‚"
    
    return prompt

# ä½¿ç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«
def identify_intent_with_llm(question, llm_client):
    intent_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜çš„æ„å›¾ï¼Œåªè¿”å›ä¸‹åˆ—ä¹‹ä¸€ï¼š
- query: æŸ¥è¯¢æ•°æ®
- analysis: æ•°æ®åˆ†æ
- reject: æ¶‰åŠæ•°æ®åº“å¢åˆ æ”¹æ“ä½œï¼ˆINSERT/UPDATE/DELETE/DROP/CREATE/ALTERç­‰ï¼‰

ç”¨æˆ·é—®é¢˜: {question}

æ„å›¾:"""
    
    try:
        response = llm_client.generate_sql(intent_prompt)
        if response:
            response_lower = response.lower().strip()
            if "reject" in response_lower:
                return "reject"
            elif "analysis" in response_lower:
                return "analysis"
            else:
                return "query"
    except:
        pass
    
    return "query"  # é»˜è®¤è¿”å›æŸ¥è¯¢

# ç”Ÿæˆåˆ†ææ€è·¯
def generate_analysis_plan(question, schema_info, table_descriptions, llm_client):
    plan_prompt = f"""è¯·ä¸ºä»¥ä¸‹æ•°æ®åˆ†æé—®é¢˜åˆ¶å®šè¯¦ç»†çš„åˆ†ææ€è·¯å’Œæ­¥éª¤ï¼š

ç”¨æˆ·é—®é¢˜: {question}

æ•°æ®åº“è¡¨ç»“æ„:
"""
    
    for table, columns in schema_info.items():
        table_desc = table_descriptions.get(table, "")
        plan_prompt += f"\nè¡¨: {table}"
        if table_desc:
            plan_prompt += f" - {table_desc}"
        plan_prompt += "\n"
        
        if columns:
            for col in columns:
                name = col.get("name", "")
                col_type = col.get("type", "")
                comment = col.get("comment", "")
                plan_prompt += f"  - {name} ({col_type}): {comment}\n"
    
    plan_prompt += "\n\nè¯·æä¾›ä¸€ä¸ªåˆ†æ­¥éª¤çš„åˆ†æè®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š\n1. åˆ†æç›®æ ‡\n2. æ‰€éœ€æ•°æ®\n3. åˆ†ææ­¥éª¤\n4. é¢„æœŸç»“æœ"
    
    try:
        response = llm_client.generate_sql(plan_prompt)
        return response
    except Exception as e:
        return f"ç”Ÿæˆåˆ†æè®¡åˆ’æ—¶å‡ºé”™: {str(e)}"

# ä½¿ç”¨LLMæ£€æµ‹æ˜¯å¦ä¸ºæ‰§è¡Œæ„å›¾
def is_execute_intent_with_llm(question, llm_client):
    execute_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥æ˜¯å¦è¡¨ç¤ºè¦æ‰§è¡Œå½“å‰çš„åˆ†æè®¡åˆ’ï¼š

ç”¨æˆ·è¾“å…¥: {question}

è¯·åªè¿”å›ï¼š
- execute: ç”¨æˆ·è¦æ±‚æ‰§è¡Œåˆ†æè®¡åˆ’
- modify: ç”¨æˆ·è¦æ±‚ä¿®æ”¹æˆ–è¡¥å……è®¡åˆ’

æ„å›¾:"""
    
    try:
        response = llm_client.generate_sql(execute_prompt)
        if response and "execute" in response.lower():
            return True
    except:
        pass
    
    return False

# æ£€æµ‹SQLä¸­çš„å±é™©æ“ä½œ
def check_dangerous_sql_operations(sql):
    dangerous_keywords = [
        'INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 
        'TRUNCATE', 'REPLACE', 'MERGE', 'GRANT', 'REVOKE'
    ]
    
    sql_upper = sql.upper().strip()
    for keyword in dangerous_keywords:
        if keyword in sql_upper:
            return True, keyword
    return False, None

# SQLç”Ÿæˆå‡½æ•°
def generate_sql(question, database_type, config_manager, llm_client=None, use_llm=False):
    # è·å–ä¿å­˜çš„schemaä¿¡æ¯
    schema_info, table_descriptions = get_saved_schema(config_manager, database_type)
    
    # æ„å»ºåŒ…å«schemaçš„prompt
    schema_prompt = build_schema_prompt(question, schema_info, table_descriptions)
    
    # ä½¿ç”¨LLMç”ŸæˆSQL
    sql = ""
    if use_llm and llm_client:
        try:
            # è°ƒç”¨LLM APIç”ŸæˆSQL
            llm_response = llm_client.generate_sql(schema_prompt)
            if llm_response:
                # ä»å“åº”ä¸­æå–SQL
                sql_match = re.search(r'```sql\s*([\s\S]*?)\s*```', llm_response)
                if sql_match:
                    sql = sql_match.group(1).strip()
                else:
                    # å°è¯•å…¶ä»–SQLä»£ç å—æ ¼å¼
                    sql_match = re.search(r'```\s*([\s\S]*?)\s*```', llm_response)
                    if sql_match:
                        sql = sql_match.group(1).strip()
                    else:
                        # å¦‚æœæ²¡æœ‰SQLä»£ç å—ï¼Œå°è¯•ç›´æ¥æå–
                        sql = llm_response.strip()
        except Exception as e:
            print(f"LLMç”ŸæˆSQLæ—¶å‡ºé”™: {str(e)}")
    
    # å¦‚æœæ²¡æœ‰ä½¿ç”¨LLMæˆ–LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç”ŸæˆSQL
    if not sql:
        tables = list(schema_info.keys())
        for table in tables:
            if table.lower() in question.lower():
                sql = f"SELECT * FROM {table} LIMIT 10"
                break
        
        if not sql and ("æŸ¥è¯¢" in question or "æ˜¾ç¤º" in question or "select" in question.lower()):
            if tables:
                sql = f"SELECT * FROM {tables[0]} LIMIT 10"
    
    # è¿”å›ç”Ÿæˆçš„SQLå’ŒåŒ…å«schemaçš„prompt
    return sql, schema_prompt

# èŠå¤©è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            if not db_config:
                response = "è¯·å…ˆåœ¨æ•°æ®åº“é…ç½®é¡µé¢é…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯"
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„schema
                schema_info, table_descriptions = get_saved_schema(config_manager, database_type)
                
                if not schema_info:
                    response = f"æœªæ‰¾åˆ°{database_type.upper()}çš„Schemaé…ç½®ï¼Œè¯·å…ˆåœ¨Schemaé…ç½®é¡µé¢é…ç½®è¡¨ç»“æ„"
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                else:
                    # æ£€æŸ¥æ˜¯å¦åœ¨åˆ†æè®¡åˆ’é˜¶æ®µ
                    if st.session_state.analysis_plan:
                        # åœ¨åˆ†æè®¡åˆ’é˜¶æ®µï¼Œæ£€æµ‹æ˜¯å¦ä¸ºæ‰§è¡Œæ„å›¾
                        if use_llm and llm_client and is_execute_intent_with_llm(prompt, llm_client):
                            intent = "analysis_execute"
                        else:
                            intent = "analysis_modify"
                    else:
                        # æ­£å¸¸æ„å›¾è¯†åˆ«
                        if use_llm and llm_client:
                            intent = identify_intent_with_llm(prompt, llm_client)
                        else:
                            intent = "query"
                    
                    intent_map = {
                        "query": "æŸ¥è¯¢æ„å›¾",
                        "analysis": "åˆ†ææ„å›¾",
                        "analysis_execute": "åˆ†ææ„å›¾ - æ‰§è¡Œé˜¶æ®µ",
                        "analysis_modify": "åˆ†ææ„å›¾ - ä¿®æ”¹é˜¶æ®µ",
                        "reject": "æ‹’ç»æ„å›¾"
                    }
                    
                    # å¦‚æœæ˜¯æ‹’ç»æ„å›¾ï¼Œç›´æ¥è¿”å›æ‹’ç»ä¿¡æ¯
                    if intent == "reject":
                        response = f"[æ„å›¾è¯†åˆ«] æ‹’ç»æ„å›¾\n\næŠ±æ­‰ï¼Œä¸ºäº†æ•°æ®å®‰å…¨ï¼Œç³»ç»Ÿä¸æ”¯æŒæ•°æ®åº“çš„å¢åˆ æ”¹æ“ä½œã€‚\nåªæ”¯æŒæ•°æ®æŸ¥è¯¢å’Œåˆ†æåŠŸèƒ½ã€‚"
                        st.markdown(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    elif intent == "analysis_execute":
                        # æ‰§è¡Œåˆ†æè®¡åˆ’
                            # æ‰§è¡Œåˆ†æè®¡åˆ’
                            sql, schema_prompt = generate_sql(st.session_state.analysis_question, database_type, config_manager, llm_client, use_llm)
                            
                            if sql:
                                # æ£€æµ‹å±é™©SQLæ“ä½œ
                                if check_dangerous_sql:
                                    is_dangerous, dangerous_keyword = check_dangerous_sql_operations(sql)
                                    if is_dangerous:
                                        response = f"[å®‰å…¨æ£€æµ‹] æ£€æµ‹åˆ°å±é™©æ“ä½œ\n\næ£€æµ‹åˆ°SQLä¸­åŒ…å«å±é™©æ“ä½œ: {dangerous_keyword}\nä¸ºäº†æ•°æ®å®‰å…¨ï¼Œç³»ç»Ÿæ‹’ç»æ‰§è¡Œæ­¤æŸ¥è¯¢ã€‚\n\nç”Ÿæˆçš„SQL:\n```sql\n{sql}\n```"
                                        st.markdown(response)
                                        st.session_state.messages.append({"role": "assistant", "content": response})
                                        st.stop()
                                
                                # æ‰§è¡Œåˆ†ææŸ¥è¯¢
                                response_parts = []
                                response_parts.append(f"[æ„å›¾è¯†åˆ«] åˆ†ææ„å›¾ - æ‰§è¡Œé˜¶æ®µ")
                                response_parts.append(f"æ­£åœ¨æ‰§è¡Œåˆ†æ: {st.session_state.analysis_question}")
                                response_parts.append(f"æ•°æ®åº“: {database_type}")
                                response_parts.append(f"SQL: \n```sql\n{sql}\n```")
                                
                                response = "\n\n".join(response_parts)
                                st.markdown(response)
                                
                                # æ‰§è¡ŒæŸ¥è¯¢
                                with st.spinner("æ‰§è¡Œåˆ†ææŸ¥è¯¢ä¸­..."):
                                    query_result = mcp_client.call_mcp_server_with_config(
                                        database_type,
                                        "execute_query",
                                        db_config,
                                        {"sql": sql, "database": db_config.get("database")}
                                    )
                                    
                                    if "error" in query_result:
                                        st.error(f"æŸ¥è¯¢å¤±è´¥: {query_result['error']}")
                                        st.session_state.messages.append({"role": "assistant", "content": response + "\n\næŸ¥è¯¢å¤±è´¥: " + query_result['error']})
                                    elif "result" in query_result and "data" in query_result["result"]:
                                        columns = query_result["result"]["data"].get("columns", [])
                                        rows = query_result["result"]["data"].get("rows", [])
                                        
                                        if rows:
                                            df = pd.DataFrame(rows, columns=columns)
                                            st.dataframe(df)
                                            st.session_state.messages.append({
                                                "role": "assistant", 
                                                "content": response,
                                                "data": df
                                            })
                                        else:
                                            st.info("æŸ¥è¯¢ç»“æœä¸ºç©º")
                                            st.session_state.messages.append({"role": "assistant", "content": response + "\n\næŸ¥è¯¢ç»“æœä¸ºç©º"})
                                    else:
                                        st.warning("æŸ¥è¯¢ç»“æœæ ¼å¼ä¸æ­£ç¡®")
                                        st.session_state.messages.append({"role": "assistant", "content": response + "\n\næŸ¥è¯¢ç»“æœæ ¼å¼ä¸æ­£ç¡®"})
                                
                                # æ¸…é™¤åˆ†æè®¡åˆ’
                                st.session_state.analysis_plan = None
                                st.session_state.analysis_question = None
                            else:
                                response = "æ— æ³•ç”Ÿæˆåˆ†æSQLæŸ¥è¯¢"
                                st.markdown(response)
                                st.session_state.messages.append({"role": "assistant", "content": response})
                    elif intent == "analysis_modify":
                        # ä¿®æ”¹åˆ†æè®¡åˆ’
                        if use_llm and llm_client:
                            # æ›´æ–°åˆ†æè®¡åˆ’
                            updated_plan = generate_analysis_plan(f"{st.session_state.analysis_question}\n\nç”¨æˆ·è¡¥å……: {prompt}", schema_info, table_descriptions, llm_client)
                            st.session_state.analysis_plan = updated_plan
                            
                            response = f"[æ„å›¾è¯†åˆ«] åˆ†ææ„å›¾ - ä¿®æ”¹é˜¶æ®µ\n\nå·²æ ¹æ®æ‚¨çš„è¡¥å……æ›´æ–°åˆ†æè®¡åˆ’ï¼š\n\n{updated_plan}\n\n---\n\nè¯·è¾“å…¥â€œæ‰§è¡Œâ€æˆ–ç±»ä¼¼æ„æ€æ¥å¼€å§‹åˆ†æã€‚"
                            st.markdown(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})
                        else:
                            response = "è¯·å…ˆå¯ç”¨LLMåŠŸèƒ½æ‰èƒ½è¿›è¡Œæ•°æ®åˆ†æ"
                            st.markdown(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})
                    elif intent == "analysis":
                        # ç”Ÿæˆåˆ†æè®¡åˆ’
                        if use_llm and llm_client:
                            analysis_plan = generate_analysis_plan(prompt, schema_info, table_descriptions, llm_client)
                            st.session_state.analysis_plan = analysis_plan
                            st.session_state.analysis_question = prompt
                            
                            response = f"[æ„å›¾è¯†åˆ«] åˆ†ææ„å›¾ - è®¡åˆ’é˜¶æ®µ\n\nä»¥ä¸‹æ˜¯ä¸ºæ‚¨çš„åˆ†æé—®é¢˜åˆ¶å®šçš„è®¡åˆ’ï¼š\n\n{analysis_plan}\n\n---\n\nè¯·è¾“å…¥â€œæ‰§è¡Œâ€æˆ–ç±»ä¼¼æ„æ€æ¥å¼€å§‹åˆ†æã€‚"
                            st.markdown(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})
                        else:
                            response = "è¯·å…ˆå¯ç”¨LLMåŠŸèƒ½æ‰èƒ½è¿›è¡Œæ•°æ®åˆ†æ"
                            st.markdown(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})
                    else:
                        # ç”ŸæˆSQLæŸ¥è¯¢
                        sql, schema_prompt = generate_sql(prompt, database_type, config_manager, llm_client, use_llm)
                        
                        if sql:
                            # æ£€æµ‹å±é™©SQLæ“ä½œ
                            if check_dangerous_sql:
                                is_dangerous, dangerous_keyword = check_dangerous_sql_operations(sql)
                                if is_dangerous:
                                    response = f"[å®‰å…¨æ£€æµ‹] æ£€æµ‹åˆ°å±é™©æ“ä½œ\n\næ£€æµ‹åˆ°SQLä¸­åŒ…å«å±é™©æ“ä½œ: {dangerous_keyword}\nä¸ºäº†æ•°æ®å®‰å…¨ï¼Œç³»ç»Ÿæ‹’ç»æ‰§è¡Œæ­¤æŸ¥è¯¢ã€‚\n\nç”Ÿæˆçš„SQL:\n```sql\n{sql}\n```"
                                    st.markdown(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                                    st.stop()
                            
                            # æ„å»ºå“åº”
                            response_parts = []
                            response_parts.append(f"[æ„å›¾è¯†åˆ«] {intent_map.get(intent, 'æŸ¥è¯¢æ„å›¾')}")
                            response_parts.append(f"æ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢: {prompt}")
                            response_parts.append(f"æ•°æ®åº“: {database_type}")
                            
                            # æ ¹æ®è®¾ç½®æ˜¾ç¤ºLLMä¿¡æ¯
                            if use_llm:
                                if provider == "openai":
                                    response_parts.append(f"LLM: OpenAI - {llm_config.get('openai', {}).get('model', 'gpt-4')}")
                                elif provider == "azure_openai":
                                    response_parts.append(f"LLM: Azure OpenAI - {llm_config.get('azure_openai', {}).get('deployment_name', '')}")
                                else:  # custom
                                    response_parts.append(f"LLM: è‡ªå®šä¹‰ - {llm_config.get('custom', {}).get('model', 'llama2')}")
                            
                            # æ ¹æ®è®¾ç½®æ˜¾ç¤ºSchemaæç¤º
                            if show_schema:
                                response_parts.append(f"æ•°æ®åº“Schemaæç¤º:\n```\n{schema_prompt}\n```")
                            
                            # æ·»åŠ SQL - ä½¿ç”¨æ›´å®‰å…¨çš„æ ¼å¼åŒ–æ–¹å¼
                            response_parts.append(f"SQL: ")  # å…ˆæ·»åŠ æ ‡ç­¾
                            response_parts.append(f"```sql")  # å•ç‹¬ä¸€è¡Œå¼€å§‹ä»£ç å—
                            response_parts.append(sql)       # æ·»åŠ SQLä»£ç 
                            response_parts.append(f"```")    # å•ç‹¬ä¸€è¡Œç»“æŸä»£ç å—
                            
                            # ç»„åˆå“åº”
                            response = "\n\n".join(response_parts)
                            try:
                                st.markdown(response)
                            except Exception as e:
                                # å¦‚æœmarkdownæ¸²æŸ“å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨çº¯æ–‡æœ¬æ˜¾ç¤º
                                st.text(f"Markdownæ¸²æŸ“å¤±è´¥ï¼Œä»¥ä¸‹æ˜¯åŸå§‹å“åº”:\n{response}")
                                st.error(f"æ¸²æŸ“é”™è¯¯: {str(e)}")
                            
                            # æ‰§è¡ŒæŸ¥è¯¢
                            with st.spinner("æ‰§è¡ŒæŸ¥è¯¢ä¸­..."):
                                query_result = mcp_client.call_mcp_server_with_config(
                                    database_type,
                                    "execute_query",
                                    db_config,
                                    {"sql": sql, "database": db_config.get("database")}
                                )
                                
                                if "error" in query_result:
                                    st.error(f"æŸ¥è¯¢å¤±è´¥: {query_result['error']}")
                                    st.session_state.messages.append({"role": "assistant", "content": response + "\n\næŸ¥è¯¢å¤±è´¥: " + query_result['error']})
                                elif "result" in query_result and "data" in query_result["result"]:
                                    # å°†æŸ¥è¯¢ç»“æœè½¬æ¢ä¸ºDataFrame
                                    columns = query_result["result"]["data"].get("columns", [])
                                    rows = query_result["result"]["data"].get("rows", [])
                                    
                                    if rows:
                                        df = pd.DataFrame(rows, columns=columns)
                                        st.dataframe(df)
                                        st.session_state.messages.append({
                                            "role": "assistant", 
                                            "content": response,
                                            "data": df
                                        })
                                    else:
                                        st.info("æŸ¥è¯¢ç»“æœä¸ºç©º")
                                        st.session_state.messages.append({"role": "assistant", "content": response + "\n\næŸ¥è¯¢ç»“æœä¸ºç©º"})
                                else:
                                    st.warning("æŸ¥è¯¢ç»“æœæ ¼å¼ä¸æ­£ç¡®")
                                    st.session_state.messages.append({"role": "assistant", "content": response + "\n\næŸ¥è¯¢ç»“æœæ ¼å¼ä¸æ­£ç¡®"})
                        else:
                            tables = list(schema_info.keys())
                            response = f"æˆ‘æ— æ³•ç†è§£æ‚¨çš„æŸ¥è¯¢æ„å›¾ã€‚è¯·å°è¯•ä»¥ä¸‹æ ¼å¼:\n- æŸ¥è¯¢[è¡¨å]\n- æ˜¾ç¤º[è¡¨å]çš„æ•°æ®\n\nå¯ç”¨çš„è¡¨: {', '.join(tables)}"
                            st.markdown(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})

# æ¸…é™¤èŠå¤©å†å²
if st.button("æ¸…é™¤å†å²"):
    st.session_state.messages = []
    st.rerun()